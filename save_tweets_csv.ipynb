{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "#import csv\n",
    "import pandas as pd \n",
    "\n",
    "import spacy\n",
    "import geopy \n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "locator = geopy.geocoders.Nominatim(user_agent=\"mygeocoder\")\n",
    "geocode = RateLimiter(locator.geocode, min_delay_seconds=5)\n",
    "\n",
    "nlp_wk = spacy.load(\"xx_ent_wiki_sm\")\n",
    "\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAP1LRwEAAAAAn64BsczyRJiYHOVTTbN9415JH%2Fs%3DkzXwt5UPrYzTxL5sNVtqZD25ejB0UOtB4ayTcdZQfEUIwX26Qx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_userTimeline(userDetails): \n",
    "    tweet_array = []\n",
    "    for user_id in userDetails:\n",
    "        url = \"https://api.twitter.com/2/users/{}/tweets\".format(user_id)\n",
    "        payload={}\n",
    "        headers = {\n",
    "          'Authorization': 'Bearer '+bearer_token\n",
    "        }\n",
    "        response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "        for response_line in response.iter_lines():\n",
    "            if response_line:\n",
    "                json_response = json.loads(response_line)\n",
    "                #Prints loction for user's timeline\n",
    "                \n",
    "                for tweet in json_response['data']:\n",
    "                    doc = nlp_wk(tweet['text'])\n",
    "                    locations = []\n",
    "                    locations.extend([ent.text for ent in doc.ents if ent.label_ in ['LOC']])\n",
    "                    if(len(locations) > 0):\n",
    "                        tweet_array.append([tweet['id'],tweet['text'],locations])\n",
    "    tweetDatFrame = pd.DataFrame(tweet_array,columns=['author_id','text','loc'])\n",
    "    \n",
    "    tweetDatFrame = pd.DataFrame(tweet_array,columns=['author_id','text','loc'])\n",
    "    tweetDatFrame.to_csv('data/userTimelineTweets.csv',sep=',', mode='a', index=False, encoding='utf-8',columns=['author_id','text','loc']) # Use Tab to seperate data            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream(): \n",
    "    query_melbRegions = [\"melbourne\"]\n",
    "    \n",
    "    with open('SA2Name_Melbourne.json') as f:\n",
    "        melb_regions = json.load(f)\n",
    "        for regions in melb_regions['features']:\n",
    "            query_melbRegions.append(regions['properties']['sa2_name16'])\n",
    "\n",
    "    n=10\n",
    "    region_chunks = [query_melbRegions[i:i + n] for i in range(0, len(query_melbRegions), n)]\n",
    "    tweet_array = []\n",
    "    users_array = []\n",
    "\n",
    "    for reg in region_chunks:\n",
    "        \n",
    "        reg_query = ' OR '.join(str(v) for v in reg)\n",
    "        query = reg_query +\" \"+ \"(beaches OR pubs OR stadiums OR Museu OR art Galleries OR Gardens OR Clubs OR Bars OR Restauraunt OR Hotels OR Motels OR Schools OR Universities OR Parlor OR Saloon OR Coffee Shop OR Casinos OR Hospitals)\"\n",
    "\n",
    "        tweetFields = \"tweet.fields=created_at,lang\"\n",
    "        maxResults = \"max_results=10\"\n",
    "        expansions = \"expansions=author_id,geo.place_id\"\n",
    "\n",
    "        url = \"https://api.twitter.com/2/tweets/search/recent?query=\"+query+\"&\"+tweetFields+\"&\"+maxResults+\"&\"+expansions+\"&\"\n",
    "\n",
    "        payload={}\n",
    "        headers = {\n",
    "          'Authorization': 'Bearer '+bearer_token,\n",
    "          'Cookie': 'guest_id=v1%3A162668876704133606; personalization_id=\"v1_QPzpEmtR+oVGPKUXrUpOIA==\"'\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "        print(response.status_code)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                \"Cannot get stream (HTTP {}): {}\".format(\n",
    "                    response.status_code, response.text\n",
    "                )\n",
    "            )    \n",
    "        \n",
    "        for response_line in response.iter_lines():\n",
    "            if response_line:\n",
    "                json_response = json.loads(response_line) \n",
    "                \n",
    "                for tweet in json_response['data']:\n",
    "                    doc = nlp_wk(tweet['text'])\n",
    "                    locations = []\n",
    "                    locations.extend([ent.text for ent in doc.ents if ent.label_ in ['LOC']])\n",
    "                    if(len(locations) > 0):\n",
    "                        tweet_array.append([tweet['author_id'],tweet['text'],locations])\n",
    "                        users_array.append(tweet['author_id'])\n",
    "    \n",
    "            \n",
    "    tweetDatFrame = pd.DataFrame(tweet_array,columns=['author_id','text','loc'])\n",
    "    tweetDatFrame.to_csv('data/gatheredTweets.csv',sep=',', mode='a', index=False, encoding='utf-8',columns=['author_id','text','loc']) # Use Tab to seperate data            \n",
    "    \n",
    "    usersDatFrame = pd.DataFrame(users_array,columns=['author_id'])\n",
    "    usersDatFrame.to_csv('data/userIds.csv',sep=',', mode='a', index=False, encoding='utf-8',columns=['author_id']) # Use Tab to seperate data            \n",
    "\n",
    "    get_userTimeline(users_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "Doing hard work here i=0\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "get_stream() \n",
    "import time\n",
    "for i in range(1000):\n",
    "    print('Doing hard work here i=' + str(i))\n",
    "    get_stream() \n",
    "    print('Taking a nap now...')\n",
    "    time.sleep(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
